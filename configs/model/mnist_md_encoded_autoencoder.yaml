_target_: models.mnist_md_encoded_autoencoder_pl.MNISTMDEncodedAutoencoderPL

defaults:
  - scheduler_config: reduce_on_plateau # options: reduce_on_plateau, linear, polynomial
  - optimizer: adam # options: adamw, adam
  - autoencoder: mlp_ae
  - additional_logger: null # reconstruction_logger


top_k: 5
num_domains: ${datamodule.num_domains}
z_dim: 128
z_dim_invariant_fraction: 0.5
z_dim_invariant: ${mult_int:${model.z_dim},${model.z_dim_invariant_fraction}}
penalty_criterion:
  minmax: 0.
  stddev: 0.
  mmd: 0.0
  domain_classification: 0.
loss_transform: "mse"
penalty_weight: 1.0
stddev_threshold: 1.0
stddev_eps: 0.0001
hinge_loss_weight: 0.0
wait_steps: 0 # 500 # 2000
linear_steps: 1 # 2000 # 3000

mmd_loss:
  _target_: models.utils.MMD_loss
  kernel_multiplier: 1.0
  kernel_number: 1
  fix_sigma: 1.0

# full ckpt containing state_dict, callbacks, optimizer, hyperparameters, etc.
pl_model_ckpt_path: null
# set this to null to train from scratch.  NOTE: This path (.pt) only contains the 
# state_dict (to be flexible), if the full ckpt is required (not flexible w/ 
# different # of slots, then load .ckpt file)
autoencoder_ckpt_path: ${retrieve_encoder_state_dict:${model.pl_model_ckpt_path}}
autoencoder_freeze: False


logging_name: "autoencoder_encoded_${datamodule.datamodule_name}_${model.autoencoder.latent_dim}"
