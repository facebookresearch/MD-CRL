scheduler:
    _target_: transformers.get_polynomial_decay_schedule_with_warmup
    num_warmup_steps: 1000 # 3000, 1000
    num_training_steps: 3000 # 10000, 2000
    lr_end: 0.00004
    
    
scheduler_dict:
    interval: "step"  # The unit of the scheduler's step size. 'step' or 'epoch
    frequency: 1  # corresponds to updating the learning rate after every `frequency` epoch/step
    name: "polynomial_decay_schedule_with_warmup"
