scheduler:
    _target_: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
    num_warmup_steps: 1000 # 3000
    num_training_steps: 3000 # 4000
    num_cycles: 5
    lr_end: 0.0
    
    
scheduler_dict:
    interval: "step"  # The unit of the scheduler's step size. 'step' or 'epoch
    frequency: 1  # corresponds to updating the learning rate after every `frequency` epoch/step
    name: "cosine_with_hard_restarts"